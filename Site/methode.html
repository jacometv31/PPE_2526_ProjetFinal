<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Méthode - Projet ESPOIR</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@1.0.4/css/bulma.min.css">
    <style>
        :root {
            --bg-gradient: linear-gradient(135deg, #e0f2fe 0%, #dbeafe 50%, #f0f9ff 100%);
            --accent-blue: #1e3a8a;
            --soft-blue: #3b82f6;
        }

        body {
            background: var(--bg-gradient);
            min-height: 100vh;
            background-attachment: fixed;
            font-family: 'Segoe UI', Roboto, sans-serif;
        }

        .hero {
            background: transparent !important;
            position: relative;
            overflow: hidden;
            padding: 3rem 0;
        }

        .hero::before {
            content: "";
            position: absolute;
            top: 0; left: 0; right: 0; bottom: 0;
            background-image: radial-gradient(var(--soft-blue) 1px, transparent 1px);
            background-size: 30px 30px;
            opacity: 0.15;
            z-index: 0;
        }

        .hero .title {
            font-size: 5rem !important;
            letter-spacing: 12px;
            text-transform: uppercase;
            font-weight: 900;
            color: var(--accent-blue);
            position: relative;
            display: inline-block;
            text-shadow: 0 0 20px rgba(255, 255, 255, 0.8), 0 0 40px rgba(59, 130, 246, 0.2);
            animation: titleFloat 4s ease-in-out infinite;
            z-index: 2;
        }

        @keyframes titleFloat {
            0%, 100% { transform: translateY(0); letter-spacing: 12px; }
            50% { transform: translateY(-8px); letter-spacing: 15px; }
        }

        .navbar.is-link {
            background-color: rgba(30, 58, 138, 0.9) !important;
            backdrop-filter: blur(10px);
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }

        .box {
            background: rgba(255, 255, 255, 0.6) !important;
            backdrop-filter: blur(12px);
            border-radius: 24px;
            border: 1px solid rgba(255, 255, 255, 0.4);
            border-left: 8px solid var(--soft-blue);
            box-shadow: 0 10px 30px rgba(0,0,0,0.03);
        }

        .section-title {
            color: var(--accent-blue);
            font-weight: 800;
            text-align: center;
            margin-bottom: 2rem;
        }

        .method-step { margin-bottom: 2.5rem; }
        .step-number {
            background: var(--accent-blue);
            color: white;
            padding: 5px 15px;
            border-radius: 50px;
            font-size: 1rem;
            margin-right: 15px;
        }
        .step-title {
            color: var(--accent-blue);
            font-weight: bold;
            font-size: 1.4rem;
            display: flex;
            align-items: center;
            margin-bottom: 1rem;
        }
        .sub-step {
            margin-left: 2rem;
            border-left: 2px solid var(--soft-blue);
            padding-left: 1.5rem;
        }

        .tab-content { display: none; }
        .tab-content.is-active { display: block; }

        footer {
            padding: 2rem 0;
            background: transparent;
            color: var(--accent-blue);
            opacity: 0.7;
        }
    </style>
</head>
<body>

    <section class="hero is-link">
        <div class="hero-body has-text-centered">
            <p class="title">ESPOIR</p>
            <p class="subtitle is-4" style="color: var(--accent-blue) !important; opacity: 0.8;">
                Méthodologie du projet
            </p>
        </div>
    </section>

    <nav class="navbar is-link" role="navigation" aria-label="main navigation">
        <div class="container">
            <div class="navbar-brand">
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarMain">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div id="navbarMain" class="navbar-menu">
                <div class="navbar-start">
                    <a class="navbar-item" href="../index.html">Accueil</a>
                    <a class="navbar-item" href="equipe.html">Qui sommes-nous</a>
                    <a class="navbar-item is-active">Méthode</a>
                    <a class="navbar-item" href="../Partie_Japonais/Page_Tableaux.html">Tableaux</a>
                    <a class="navbar-item" href="analyse.html">Analyse</a>
                    <a class="navbar-item" href="nuages.html">Nuages de mots</a>
                    <a class="navbar-item" href="conclusion.html" style="font-weight: bold;">Conclusion</a>
                </div>
            </div>
        </div>
    </nav>

    <section class="section">
        <div class="container">
            <h1 class="title section-title">Étapes de réalisation par langue</h1>

            <div class="tabs is-centered is-boxed is-medium">
                <ul>
                    <li class="is-active" data-tab="arabe"><a>Arabe</a></li>
                    <li data-tab="francais"><a>Français</a></li>
                    <li data-tab="japonais"><a>Japonais</a></li>
                </ul>
            </div>

            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    
                    <div id="arabe" class="box p-6 tab-content is-active">
                        <div class="content">
                            <div class="method-step">
                                <div class="step-title"><span class="step-number">1</span> Constitution du corpus initial</div>
                                <div class="sub-step">
                                    <p><strong>Sélection ciblée :</strong> Recherche et sélection manuelle de 50 URLs pertinentes en langue arabe autour du mot pôle <strong>أمل</strong> (espoir).</p>
                                </div>
                            </div>

                            <div class="method-step">
                                <div class="step-title"><span class="step-number">2</span> Développement du script principal et structuration</div>
                                <div class="sub-step">
                                    <p><strong>Automatisation du tableau :</strong> Écriture d'un script Bash pour générer dynamiquement un tableau HTML de bord.</p>
                                    <p><strong>Chaîne de traitement :</strong> Mise en place d'une boucle d'aspiration avec <code>curl</code>, conversion des pages en texte brut via <code>lynx</code> et extraction automatisée des contextes avec <code>grep</code>.</p>
                                    <p><strong>Gestion de l'arborescence :</strong> Création automatique des dossiers de stockage (aspirations, dumps-text, contextes,etc...) pour une organisation rigoureuse des données.</p>
                                </div>
                            </div>

                            <div class="method-step">
                                <div class="step-title"><span class="step-number">3</span> Analyse Quantitative (PALS)</div>
                                <div class="sub-step">
                                    <p><strong>Préparation des données :</strong> Utilisation du script make_pals_corpus.sh pour la mise en forme du texte (tokenisation) afin de générer les fichiers consolidés des contextes et des dumps.</p>
                                    <p><strong>Exploration statistique :</strong> Application des outils PALS pour calculer les indices de spécificité et identifier les termes les plus significatifs associés au mot pôle.</p>
                                </div>
                            </div>

                            <div class="method-step">
                                <div class="step-title"><span class="step-number">4</span> Visualisation (Wordcloud)</div>
                                <div class="sub-step">
                                    <p><strong>Synthèse graphique :</strong> Génération d'un nuage de mots basé sur les fréquences pour identifier visuellement les thématiques sémantiques majeures liées au mot <strong>أمل</strong>.</p>
                                </div>
                            </div>

                            <div class="method-step">
                                <div class="step-title"><span class="step-number">5</span> Analyse Qualitative (iTrameur)</div>
                                <div class="sub-step">
                                    <p><strong>Focus sémantique :</strong> Importation du fichier de <strong>contextes arabes uniquement</strong> dans l'outil iTrameur.</p>
                                    <p><strong>Étude textométrique :</strong> Analyse approfondie des concordances pour observer le comportement réel du mot pôle dans son environnement textuel.</p>
                                </div>
                            </div>

                            <div class="method-step">
                                <div class="step-title"><span class="step-number">6</span> Éthique et Conformité (Robots.txt)</div>
                                <div class="sub-step">
                                    <p><strong>Respect de la courtoisie :</strong> Développement d’un script d’analyse des fichiers <code>robots.txt</code> pour vérifier les autorisations de moissonnage des serveurs.</p>
                                    <p><strong>Séparation des tableaux :</strong> Utilisation d'une blacklist et génération d'un <strong>second tableau de contrôle indépendant</strong> pour séparer clairement les URLs autorisées des URLs interdites.</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div id="francais" class="box p-6 tab-content">
    <h2 class="title is-4">Méthodologie - Français</h2>
    <div class="content">

        <!-- Introduction générale -->
        <div class="method-step">
            <div class="step-title"><span class="step-number">0</span> Introduction</div>
            <div class="sub-step">
                <p>Ce projet s’inscrit dans le cadre du cours <strong>Programmation et Projet Encadré</strong> et a pour objectif de mettre en œuvre une chaîne complète de collecte, de traitement et d’analyse textométrique de données issues du Web. Le mot étudié est <strong>« espoir »</strong>, choisi comme pôle lexical central de l’analyse.</p>
                <p>L’ensemble du travail repose sur une approche progressive, combinant des scripts bash pour le moissonnage et le prétraitement des données, puis des scripts Python pour l’analyse textométrique et la visualisation.</p>
            </div>
        </div>

        <!-- Étape 1 -->
        <div class="method-step">
            <div class="step-title"><span class="step-number">1</span> Constitution du corpus et organisation des données</div>
            <div class="sub-step">
                <p> Comme c'était le cas dans le miniprojet vu en cours le point de départ du projet est un fichier d’URLS contenant les pages web à analyser tout en suivant une architecure bien précise qui a été mise en place :</p>
                <ul>
                    <li><strong>URLs/</strong> : fichiers d’URLs sources</li>
                    <li><strong>aspirations/</strong> : pages HTML récupérées</li>
                    <li><strong>dumps-text/</strong> : versions textuelles des pages</li>
                    <li><strong>contextes/</strong> : contextes autour du mot étudié</li>
                    <li><strong>concordances/</strong> : concordanciers HTML</li>
                    <li><strong>tableaux/</strong> : tableaux récapitulatifs HTML</li>
                    <li><strong>pals/</strong> : corpus formatés pour les scripts PALS</li>
                </ul>
                <p>Cette structuration permet une traçabilité claire des données et facilite les vérifications à chaque étape.</p>
            </div>
        </div>

        <!-- Étape 2 -->
        <div class="method-step">
            <div class="step-title"><span class="step-number">2</span> Moissonnage des pages web</div>
            <div class="sub-step">
                <p>Pour chaque URL du fichier source, un script bash principal a été développé afin d’automatiser le traitement. Grace à l’utilisation de la commande <code>curl</code> on aréussi à :</p>
                <ul>
                    <li>récupérer la page HTML</li>
                    <li>suivre les redirections éventuelles</li>
                    <li>stocker localement le contenu</li>
                    <li>récupérer le code HTTP afin de vérifier le succès de la requête</li>
                </ul>
                <p>Seules les pages ayant retourné un code HTTP 200 sont conservées pour la suite du traitement.</p>
            </div>
        </div>

       

        <!-- Étape 3 -->
        <div class="method-step">
            <div class="step-title"><span class="step-number">3</span> Normalisation de l’encodage et extraction du texte</div>
            <div class="sub-step">
                <p>Les pages HTML récupérées peuvent présenter des encodages variés, le script détecte automatiquement l’encodage avec la commande <code>file</code>. Si la page est déjà en UTF-8, le texte est directement extrait, sinon une conversion est effectuée avec <code>iconv</code>.</p>
                <p>L’extraction du texte brut est réalisée avec <code>lynx -dump</code> qu'on a vu dans les cours précédents afin d’obtenir un contenu exploitable pour l’analyse linguistique.</p>
            </div>
        </div>

        <!-- Étape 4 -->
        <div class="method-step">
            <div class="step-title"><span class="step-number">4</span> Extraction des contextes et création des concordances</div>
            <div class="sub-step">
                <p>À partir des fichiers textuels, les contextes du mot « espoir » sont extraits à l’aide de <code>egrep</code>. Un concordancier HTML est ensuite généré pour chaque document, présentant :</p>
                <ul>
                    <li>le contexte gauche</li>
                    <li>le mot étudié </li>
                    <li>le contexte droit</li>
                </ul>
                <p>Ce format facilite l’analyse qualitative et permet d’observer les usages discursifs du mot dans différents contextes.</p>
            </div>
        </div>

        <!-- Étape 5 -->
        <div class="method-step">
            <div class="step-title"><span class="step-number">5</span> Construction du tableau récapitulatif HTML</div>
            <div class="sub-step">
                <p>Pour chaque URL traitée, une ligne est ajoutée à un tableau HTML global contenant :</p>
                <ul>
                    <li>l’URL source</li>
                    <li>le statut robots.txt</li>
                    <li>le code HTTP</li>
                    <li>l’encodage</li>
                    <li>le nombre d’occurrences du mot étudié</li>
                    <li>des liens vers les fichiers générés (HTML, texte, contextes, concordances)</li>
                </ul>
                <p>Ce tableau constitue une vue générale du corpus et permet de naviguer facilement entre les différents niveaux de données.</p>
            </div>
        </div>

        <!-- Étape 6 -->
        <div class="method-step">
            <div class="step-title"><span class="step-number">6</span> Préparation des données pour l’analyse textométrique (PALS)</div>
            <div class="sub-step">
                <p>Les fichiers de textes et de contextes ont ensuite été reformattés afin d’être compatibles avec les scripts PALS. Un script bash (<code>make_pals_corpus.sh</code>) a été fait pour :</p>
                <ul>
                    <li>itérer sur les fichiers textuels ou contextuels</li>
                    <li>effectuer une tokenisation </li>
                    <li>produire des fichiers au format attendu par PALS (un mot par ligne)</li>
                </ul>
                <p>Ces fichiers servent de base aux analyses quantitatives.</p>
            </div>
        </div>

        <!-- Étape 7 -->
        <div class="method-step">
            <div class="step-title"><span class="step-number">7</span> Analyse lexicale et nuage de mots</div>
            <div class="sub-step">
                <p>À partir des contextes du mot « espoir », une analyse lexicale a été menée pour identifier les mots les plus fréquents autour du pôle. Un script Python a été récupéré depuis des cours pour générer un nuage de mots, en excluant les stopwords afin de faire ressortir les termes sémantiquement pertinents. Le second nuage de mot a été généré depuis les résultats du script cooccurents.py (Spécificités de Lafon </p>
                <p>Le mot « espoir » est volontairement conservé et placé au centre de la visualisation, entouré de ses cooccurrents les plus fréquents.</p>
            </div>
        </div>
        

     </div>
</div>


                    <div id="japonais" class="box p-6 tab-content">
                        <div class="content">
                            <div class="method-step">
                                <div class="step-title"><span class="step-number">1</span>Création du corpus</div>
                                <div class="sub-step">
                                    <p>Pour ce corpus en langue japonaise, j'ai réaliser une sélection manuelle de 30 URLs pertinentes autour du mot <strong>希望</strong>(espoir dans le sens du souhait) ainsi que de 30 autres URLs pertinentes autour du mot <strong>期待</strong>(espoir dans le sens d'attente).</p>
                                </div>
                            </div>

                            <div class="method-step">
                                <div class="step-title"><span class="step-number">2</span> Développement des scripts pour les tableaux</div>
                                <div class="sub-step">
                                    <p>Tout d'abord, afin de nous prépaper pour travail, nous avions réaliser pendant les cours un script permettant à partir d'un fichier texte comprennant des URLs, d'afficher sur une page html, un tableau avec diverse colonnes. Ce tableau comprennait des colonnes indiquant pour chaque URL son numéro de ligne, son code HTTP, l'affichage de l'URL complète, son encodage ainsi que le nombre de mot présent sur la page web que renvoie l'URL.</p>
                                    <p>J'ai donc récupérer et modifier ce script afin de réaliser mes deux tableaux. Pour ce travail, plusieurs colonnes était demandé pour le tableau. D'abord, les colonnes de numéro de ligne, de code HTTP, d'affichage de l'URL ainsi que l'encodage. Pour celle-ci, j'ai simplement utilisé le code de mon ancien script. Ensuite, de nouvelles colonnes était demandé: le nombre d'occurrence de notre mot, le HTML brut de la page web, le DUMP textuel, le contexte et pour finir, un concordancier HTML. </p>
                                    <p>J'ai donc remplacer ma colonne qui indiquait le nombre de mot de la page web par le nombre d'occurrence du mot qui m'intéresse. J'ai donc utilisé la ligne de code suivante: <code>occurrence=$(curl -s "$line" | grep -o -i "希望" | wc -l)</code>. Ensuite, pour le HTML brut j'ai utilisé le code suivant: <code>lynx -source "$line" > "../html_brut/mot1/page_$i.html"</code>. Pour le DUMP textuel, j'ai utilisé ce code: <code>lynx -dump "$line" > "../dump/mot1/dump_$i.txt"</code>. Pour le contexte, j'ai rajouté la ligne suivante: <br><code>  grep -C 3 --no-group-separator "希望" \"../dump/mot1/dump_$i.txt" \ > "../contexte/mot1/contexte_$i.txt"</code>. Et enfin, pour réaliser le concordancier, j'ai utilisé le code suivant:<br> <code> sed -n " /希望/{ s/^\(.*\)\(希望\)\(.*\)$/<tr><td class=\"gauche\">\1<\/td><td class=\"mot\">\2<\/td><td class=\"droite\">\3<\/td><\/tr>/ p}" "../dump/mot1/dump_$i.txt"</code><br>(Les lignes de codes montrée ici sont celles utilisé pour le premier tableau. Pour le second tableau, les mêmes lignes de code ont été utilisé sauf que le mot "希望"(Kibō) à été remplacé par le mot "期待"(Kitai))</p>
                                </div>
                            </div>

                            <div class="method-step">
                                <div class="step-title"><span class="step-number">3</span> Analyse des tableaux</div>
                                <div class="sub-step">
                                    <p>Afin d'analyser les tableaux, j'ai d'abord réalisé un script nommé make_pals_corpus.sh qui m'a permis de rassembler ainsi que de tokéniser les fichiers de contexte présent dans mon tableau. J'ai bien entendu exécuté le script deux fois afin d'obtenir un fichier contexte_corpus.txt pour le premier tableau ainsi qu'un fichier contexte2_corpus.txt pour le second tableau.</p>
                                    <p>Grâce à cela, ainsi qu'à l'aide de fichiers python nommé cooccurrents.py et partition.py fournis par les proffesseurs, j'ai pu obtenir 3 tableaux pour réaliser mon analyse. Un premier tableau pour les cooccurrences du mot "希望"(Kibō) dans le premier corpus, un second tableau pour les cooccurrences du mot "期待"(Kitai) dans le second corpus, ainsi qu'un dernier tableau pour comparer les deux corpus.</p>
                                </div>
                            </div>

                            <div class="method-step">
                                <div class="step-title"><span class="step-number">4</span>Création des nuages de mots</div>
                                <div class="sub-step">
                                    <p>Comme j'ai travailler avec deux mots différents, j'ai décidé de réaliser deux nuages de mots différents. Pour réaliser ces nuages de mots, j'ai d'abord installé le paquet wordcloud sur l'environnement virtuel de mon ordinateur qui me permet de créer des nuages de mots à l'aide d'un script python. Le script python et wordcloud me permettent de réaliser un nuage de mots à partir d'un fichier texte. C'est pourquoi, j'ai décidé d'utilisé les fichiers contexte_corpus.txt et contexte2_corpus.txt que j'avais réalisé précédemment. Toutefois, ces nuages de mots ne me satisfaisant pas car il y avait trop de ponctuations, de lettres et de chiffres. J'ai donc décidé de créer un nouveau script afin de nettoyer mes fichiers précédent pour qu'il ne reste plus que des caractères japonais. Avec ce script, j'ai donc obtenu deux nouveaux fichiers: contexte_nettoye.txt et contexte2_nettoye.txt, que j'ai utilisé pour réaliser mes nuages de mots</p>
                                </div>
                            </div>

                        </div>
                    </div>

                </div>
            </div>
        </div>
    </section>

    <footer class="has-text-centered">
        <p><strong>Projet PPE</strong> - Master 1 TAL - 2025-2026</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            // Menu Burger Mobile
            const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);
            $navbarBurgers.forEach( el => {
                el.addEventListener('click', () => {
                    const target = el.dataset.target;
                    const $target = document.getElementById(target);
                    el.classList.toggle('is-active');
                    $target.classList.toggle('is-active');
                });
            });

            // Gestion des Onglets
            const tabs = document.querySelectorAll('.tabs li');
            const contents = document.querySelectorAll('.tab-content');

            tabs.forEach(tab => {
                tab.addEventListener('click', () => {
                    tabs.forEach(t => t.classList.remove('is-active'));
                    tab.classList.add('is-active');

                    const target = tab.dataset.tab;
                    contents.forEach(content => {
                        content.classList.remove('is-active');
                        if (content.id === target) {
                            content.classList.add('is-active');
                        }
                    });
                });
            });
        });
    </script>
</body>
</html>
